[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "This is Ben‚Äôs website.",
    "section": "",
    "text": "Just here to learn stuff."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "AboutA",
    "section": "",
    "text": "I‚Äôm really here just to learn."
  },
  {
    "objectID": "tidymodels.html",
    "href": "tidymodels.html",
    "title": "Overview of tidymodels",
    "section": "",
    "text": "I made a summary of how to use tidymodels."
  },
  {
    "objectID": "tidymodels.html#random-forest-template",
    "href": "tidymodels.html#random-forest-template",
    "title": "Overview of tidymodels",
    "section": "Random forest template",
    "text": "Random forest template\n\n# ============================================================\n# Random Forest (tidymodels): End-to-End Workflow\n# Using `parameters()` + grid generation (no expand.grid)\n# ============================================================\n\n\n# ------------------------------------------------------------\n# Assumptions\n# ------------------------------------------------------------\n# df      : your data frame\n# outcome : factor outcome variable for classification\n# Event   : second factor level is treated as the \"event\" by yardstick\n\n# ------------------------------------------------------------\n# 1. Train / Test Split\n# ------------------------------------------------------------\nsplit &lt;- initial_split(df, prop = 0.80, strata = outcome)\ntrain &lt;- training(split)\ntest  &lt;- testing(split)\n\n# ------------------------------------------------------------\n# 2. Preprocessing Recipe\n# ------------------------------------------------------------\nrf_rec &lt;- recipe(outcome ~ ., data = train) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%     # handle unseen levels\n  step_dummy(all_nominal_predictors()) %&gt;%     # one-hot encode factors\n  step_zv(all_predictors())                    # remove zero-variance predictors\n\n# ------------------------------------------------------------\n# 3. Random Forest Model Spec (tunable params declared via tune())\n# ------------------------------------------------------------\nrf_spec &lt;- rand_forest(\n  mtry  = tune(),\n  min_n = tune(),\n  trees = tune()\n) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"classification\")\n\n# ------------------------------------------------------------\n# 4. Workflow\n# ------------------------------------------------------------\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rf_rec) %&gt;%\n  add_model(rf_spec)\n\n# ------------------------------------------------------------\n# 5. Cross-Validation\n# ------------------------------------------------------------\nfolds &lt;- vfold_cv(train, v = 5, strata = outcome)\n\n# ------------------------------------------------------------\n# 6. Define parameter ranges with `parameters()`\n# ------------------------------------------------------------\n# NOTE: mtry() is data-dependent (post-recipe predictor count), so tidymodels may\n# finalize it after prepping the recipe. You can optionally set a range too.\nrf_params &lt;- parameters(\n  mtry(range = c(2, 30)),          # adjust upper bound if you expect many dummies\n  min_n(range = c(2, 40)),\n  trees(range = c(500, 2000))\n)\n\n# Build a grid from those parameter definitions\n# Option A: random grid (often better for 3+ params)\nrf_grid &lt;- grid_random(rf_params, size = 25)\n\n# Option B: regular grid (can explode quickly: levels^(#params))\n# rf_grid &lt;- grid_regular(rf_params, levels = 5)\n\n# ------------------------------------------------------------\n# 7. Hyperparameter Tuning\n# ------------------------------------------------------------\nrf_res &lt;- tune_grid(\n  rf_wf,\n  resamples = folds,\n  grid = rf_grid,\n  metrics = metric_set(roc_auc, accuracy),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Examine tuning results\ncollect_metrics(rf_res)\nshow_best(rf_res, metric = \"roc_auc\", n = 10)\n\n# ------------------------------------------------------------\n# 8. Finalize Workflow with Best Parameters\n# ------------------------------------------------------------\nbest_rf &lt;- select_best(rf_res, metric = \"roc_auc\")\nrf_final_wf &lt;- finalize_workflow(rf_wf, best_rf)\n\n# ------------------------------------------------------------\n# 9. Final Test-Set Evaluation (honest performance)\n# ------------------------------------------------------------\nrf_last &lt;- last_fit(\n  rf_final_wf,\n  split = split,\n  metrics = metric_set(roc_auc, accuracy, sens, spec)\n)\n\ncollect_metrics(rf_last)\n\nrf_test_preds &lt;- collect_predictions(rf_last)\n\n# ROC curve\nrf_test_preds %&gt;%\n  roc_curve(outcome, .pred_2) %&gt;%   # replace .pred_2 with your event class column if needed\n  autoplot()\n\n# ------------------------------------------------------------\n# 10. Fit Final Model on Training Data (for interpretation)\n# ------------------------------------------------------------\nrf_fit &lt;- fit(rf_final_wf, data = train)\n\n# Variable importance (since ranger importance = \"permutation\")\nrf_engine &lt;- extract_fit_parsnip(rf_fit)$fit\nvip::vip(rf_engine, num_features = 20)\n\n# ------------------------------------------------------------\n# (Optional) Partial Dependence Plot via DALEX (workflow-safe)\n# ------------------------------------------------------------\nlibrary(DALEX)\nlibrary(DALEXtra)\n\nexplainer &lt;- explain_tidymodels(\n  rf_fit,\n  data = train,\n  y = train$outcome,\n  verbose = FALSE\n)\n\nmodel_profile(explainer, variables = \"age\") %&gt;% plot()"
  },
  {
    "objectID": "tidymodels.html#knn-template",
    "href": "tidymodels.html#knn-template",
    "title": "Overview of tidymodels",
    "section": "kNN template",
    "text": "kNN template\n\n# ============================================================\n# k-Nearest Neighbors (tidymodels): End-to-End Workflow\n# ============================================================\n\nlibrary(tidymodels)\nset.seed(123)\n\n# ------------------------------------------------------------\n# Assumptions\n# ------------------------------------------------------------\n# df        : your data frame\n# outcome   : factor outcome variable for classification\n# NOTE: k-NN is distance-based -&gt; normalization is essential.\n\n# ------------------------------------------------------------\n# 1. Train / Test Split\n# ------------------------------------------------------------\nsplit &lt;- initial_split(df, prop = 0.80, strata = outcome)\ntrain &lt;- training(split)\ntest  &lt;- testing(split)\n\n# ------------------------------------------------------------\n# 2. Preprocessing Recipe (important for k-NN!)\n# ------------------------------------------------------------\nknn_rec &lt;- recipe(outcome ~ ., data = train) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%     # handle unseen levels\n  step_dummy(all_nominal_predictors()) %&gt;%     # one-hot encode factors\n  step_zv(all_predictors()) %&gt;%                # remove zero-variance predictors\n  step_normalize(all_numeric_predictors())     # SCALE for distances\n\n# ------------------------------------------------------------\n# 3. k-NN Model Specification (with tuning)\n# ------------------------------------------------------------\nknn_spec &lt;- nearest_neighbor(\n  neighbors   = tune(),     # k\n  dist_power  = tune(),     # 1 = Manhattan, 2 = Euclidean\n  weight_func = tune()      # how neighbors are weighted\n) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\n# ------------------------------------------------------------\n# 4. Workflow\n# ------------------------------------------------------------\nknn_wf &lt;- workflow() %&gt;%\n  add_recipe(knn_rec) %&gt;%\n  add_model(knn_spec)\n\n# ------------------------------------------------------------\n# 5. Cross-Validation\n# ------------------------------------------------------------\nfolds &lt;- vfold_cv(train, v = 5, strata = outcome)\n\n# ------------------------------------------------------------\n# 6. Parameter Ranges + Grid\n# ------------------------------------------------------------\nknn_params &lt;- parameters(\n  neighbors(range = c(3, 35)),\n  dist_power(range = c(1, 2)),\n  weight_func(values = c(\"rectangular\", \"triangular\", \"gaussian\"))\n)\n\nknn_grid &lt;- grid_regular(knn_params, levels = 5)\n# If this is too big (levels^#params), use a random grid instead:\n# knn_grid &lt;- grid_random(knn_params, size = 30)\n\n# ------------------------------------------------------------\n# 7. Hyperparameter Tuning\n# ------------------------------------------------------------\nknn_res &lt;- tune_grid(\n  knn_wf,\n  resamples = folds,\n  grid = knn_grid,\n  metrics = metric_set(roc_auc, accuracy),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Examine tuning results\ncollect_metrics(knn_res)\nshow_best(knn_res, metric = \"roc_auc\", n = 10)\n\n# ------------------------------------------------------------\n# 8. Finalize Workflow with Best Parameters\n# ------------------------------------------------------------\nbest_knn &lt;- select_best(knn_res, metric = \"roc_auc\")\n\nknn_final_wf &lt;- finalize_workflow(knn_wf, best_knn)\n\n# ------------------------------------------------------------\n# 9. Final Test-Set Evaluation (honest performance)\n# ------------------------------------------------------------\nknn_last &lt;- last_fit(\n  knn_final_wf,\n  split = split,\n  metrics = metric_set(roc_auc, accuracy, sens, spec)\n)\n\ncollect_metrics(knn_last)\n\nknn_test_preds &lt;- collect_predictions(knn_last)\n\n# ROC curve\nknn_test_preds %&gt;%\n  roc_curve(outcome, .pred_2) %&gt;%   # replace .pred_2 with your event class column if needed\n  autoplot()\n\n# ------------------------------------------------------------\n# 10. Fit Final Model on Training Data (for interpretation/usage)\n# ------------------------------------------------------------\nknn_fit &lt;- fit(knn_final_wf, data = train)\n\n# (Optional) Confusion matrix at threshold 0.5\nknn_test_preds %&gt;%\n  mutate(.pred_class = factor(if_else(.pred_2 &gt;= 0.5, levels(train$outcome)[2], levels(train$outcome)[1]),\n                              levels = levels(train$outcome))) %&gt;%\n  conf_mat(outcome, .pred_class)"
  },
  {
    "objectID": "tidymodels.html#neural-networks",
    "href": "tidymodels.html#neural-networks",
    "title": "Overview of tidymodels",
    "section": "Neural networks",
    "text": "Neural networks\n\n# ============================================================\n# Neural Net (tidymodels): End-to-End Workflow\n# Using `mlp()` + `parameters()` + tuning + test evaluation\n# Engine: nnet (lightweight, good for portfolios)\n# ============================================================\n\nlibrary(tidymodels)\nlibrary(dials)\nlibrary(rlang)\n\nset.seed(123)\n\n# ------------------------------------------------------------\n# Assumptions\n# ------------------------------------------------------------\n# df      : your data frame\n# outcome : factor outcome variable for classification\n# NOTE: Neural nets are scale-sensitive -&gt; normalization is essential.\n\n# ------------------------------------------------------------\n# 1. Train / Test Split\n# ------------------------------------------------------------\nsplit &lt;- initial_split(df, prop = 0.80, strata = outcome)\ntrain &lt;- training(split)\ntest  &lt;- testing(split)\n\n# Identify the \"event\" class (tidymodels/yardstick default is the 2nd level)\nevent_level &lt;- levels(train$outcome)[2]\nprob_col &lt;- paste0(\".pred_\", event_level)\n\n# ------------------------------------------------------------\n# 2. Preprocessing Recipe (important for neural nets!)\n# ------------------------------------------------------------\nnn_rec &lt;- recipe(outcome ~ ., data = train) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%      # handle unseen levels\n  step_dummy(all_nominal_predictors()) %&gt;%      # one-hot encode factors\n  step_zv(all_predictors()) %&gt;%                 # remove zero-variance predictors\n  step_normalize(all_numeric_predictors())      # SCALE numeric predictors\n\n# ------------------------------------------------------------\n# 3. Neural Net Model Spec (tunable params declared via tune())\n# ------------------------------------------------------------\n# `mlp()` = multilayer perceptron\n# hidden_units: size of hidden layer\n# penalty     : weight decay (regularization)\n# epochs      : training iterations\nnn_spec &lt;- mlp(\n  hidden_units = tune(),\n  penalty      = tune(),\n  epochs       = tune()\n) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\n# ------------------------------------------------------------\n# 4. Workflow\n# ------------------------------------------------------------\nnn_wf &lt;- workflow() %&gt;%\n  add_recipe(nn_rec) %&gt;%\n  add_model(nn_spec)\n\n# ------------------------------------------------------------\n# 5. Cross-Validation\n# ------------------------------------------------------------\nfolds &lt;- vfold_cv(train, v = 5, strata = outcome)\n\n# ------------------------------------------------------------\n# 6. Define parameter ranges with `parameters()`\n# ------------------------------------------------------------\n# Notes:\n# - penalty() is on a log10 scale internally (dials handles that)\n# - epochs can be modest; too high can overfit and waste time\nnn_params &lt;- parameters(\n  hidden_units(range = c(1L, 50L)),\n  penalty(range = c(-6, -1)),          # 10^-6 to 10^-1 (log10 scale)\n  epochs(range = c(50L, 400L))\n)\n\n# Random grid is usually better than regular for 3 parameters\nnn_grid &lt;- grid_random(nn_params, size = 25)\n\n# ------------------------------------------------------------\n# 7. Hyperparameter Tuning\n# ------------------------------------------------------------\nnn_res &lt;- tune_grid(\n  nn_wf,\n  resamples = folds,\n  grid = nn_grid,\n  metrics = metric_set(roc_auc, accuracy),\n  control = control_grid(save_pred = TRUE)\n)\n\n# Examine tuning results\ncollect_metrics(nn_res)\nshow_best(nn_res, metric = \"roc_auc\", n = 10)\n\n# ------------------------------------------------------------\n# 8. Finalize Workflow with Best Parameters\n# ------------------------------------------------------------\nbest_nn &lt;- select_best(nn_res, metric = \"roc_auc\")\nnn_final_wf &lt;- finalize_workflow(nn_wf, best_nn)\n\n# ------------------------------------------------------------\n# 9. Final Test-Set Evaluation (honest performance)\n# ------------------------------------------------------------\nnn_last &lt;- last_fit(\n  nn_final_wf,\n  split = split,\n  metrics = metric_set(roc_auc, accuracy, sens, spec)\n)\n\ncollect_metrics(nn_last)\n\nnn_test_preds &lt;- collect_predictions(nn_last)\n\n# ROC curve (uses the event class probability column dynamically)\nnn_test_preds %&gt;%\n  roc_curve(outcome, !!sym(prob_col)) %&gt;%\n  autoplot()\n\n# Optional: confusion matrix at 0.5 threshold (you can choose a better threshold later)\nnn_test_preds %&gt;%\n  mutate(\n    .pred_class = factor(\n      if_else(!!sym(prob_col) &gt;= 0.5, event_level, levels(train$outcome)[1]),\n      levels = levels(train$outcome)\n    )\n  ) %&gt;%\n  conf_mat(outcome, .pred_class)\n\n# ------------------------------------------------------------\n# 10. Fit Final Model on Training Data (for interpretation/usage)\n# ------------------------------------------------------------\nnn_fit &lt;- fit(nn_final_wf, data = train)\n\n# Inspect finalized model specification\nextract_spec_parsnip(nn_fit)"
  },
  {
    "objectID": "tidymodels.html#a.-objective-prediction-task-always-first",
    "href": "tidymodels.html#a.-objective-prediction-task-always-first",
    "title": "Overview of tidymodels",
    "section": "A. Objective & prediction task (always first)",
    "text": "A. Objective & prediction task (always first)\nAnswer:\nWhat problem is the model solving?\nInclude: - Outcome definition - Population - Prediction vs explanation - Unit of prediction (patient, visit, image, etc.)\nExample:\n‚ÄúWe developed a machine learning model to predict 30-day mortality among hospitalized patients using routinely collected clinical variables.‚Äù\nüö´ Don‚Äôt start with algorithms. Start with the problem."
  },
  {
    "objectID": "tidymodels.html#b.-data-study-design",
    "href": "tidymodels.html#b.-data-study-design",
    "title": "Overview of tidymodels",
    "section": "B. Data & study design",
    "text": "B. Data & study design\nAnswer:\nWhere did the data come from, and how were they used?\nInclude: - Sample size - Outcome prevalence - Train/test split - Cross-validation strategy - Any stratification or grouping\nExample:\n‚ÄúThe dataset consisted of 4,812 patients, of whom 9.6% experienced the outcome. Data were split into training (80%) and test (20%) sets, stratified by outcome. Hyperparameters were optimized using 5-fold cross-validation on the training data.‚Äù\nThis signals methodological literacy."
  },
  {
    "objectID": "tidymodels.html#c.-preprocessing-this-matters-more-than-people-think",
    "href": "tidymodels.html#c.-preprocessing-this-matters-more-than-people-think",
    "title": "Overview of tidymodels",
    "section": "C. Preprocessing (this matters more than people think)",
    "text": "C. Preprocessing (this matters more than people think)\nAnswer:\nHow was the data prepared, and was leakage avoided?\nInclude: - Handling of categorical variables - Scaling (if applicable) - Missingness handling - Zero-variance filtering - Where preprocessing occurred (inside CV!)\nExample:\n‚ÄúCategorical predictors were dummy-encoded, and predictors with zero variance were removed using a recipe-based preprocessing pipeline. All preprocessing steps were estimated within resampling folds to prevent information leakage.‚Äù\nThat last sentence is very important!"
  },
  {
    "objectID": "tidymodels.html#d.-model-specification-tuning",
    "href": "tidymodels.html#d.-model-specification-tuning",
    "title": "Overview of tidymodels",
    "section": "D. Model specification & tuning",
    "text": "D. Model specification & tuning\nAnswer:\nWhat model was fit, and how were hyperparameters chosen?\nInclude: - Algorithm - Engine/library - Tuned parameters - Optimization metric\nRandom forest example:\n‚ÄúA random forest classifier was implemented using the ranger engine. The number of candidate predictors at each split (mtry) and the minimum node size (min_n) were optimized via grid search, selecting hyperparameters that maximized cross-validated ROC AUC.‚Äù\nüö´ Don‚Äôt just say ‚Äúwe fit a random forest.‚Äù"
  },
  {
    "objectID": "tidymodels.html#e.-performance-evaluation-this-is-the-core",
    "href": "tidymodels.html#e.-performance-evaluation-this-is-the-core",
    "title": "Overview of tidymodels",
    "section": "E. Performance evaluation (this is the core)",
    "text": "E. Performance evaluation (this is the core)\nAnswer:\nHow well does the model perform on unseen data?\nAlways include: - Held-out test set results - Primary metric - Secondary metrics - Confidence intervals if possible\nExample:\n‚ÄúOn the held-out test set, the model achieved an ROC AUC of 0.82, with sensitivity of 0.74 and specificity of 0.79 at a probability threshold of 0.5.‚Äù\nOptional (but impressive): - Calibration - Decision curves - Class imbalance handling"
  },
  {
    "objectID": "tidymodels.html#f.-model-interpretation-with-restraint",
    "href": "tidymodels.html#f.-model-interpretation-with-restraint",
    "title": "Overview of tidymodels",
    "section": "F. Model interpretation (with restraint)",
    "text": "F. Model interpretation (with restraint)\nAnswer:\nHow does the model behave, and what drives predictions?\nInclude: - Variable importance (with caveats) - Partial dependence / ALE / SHAP (if used) - Clear disclaimer about causality\nExample:\n‚ÄúPermutation-based variable importance indicated that age, baseline creatinine, and oxygen saturation contributed most strongly to predictive performance. Partial dependence plots suggested nonlinear risk increases at advanced ages; however, these plots reflect model behavior rather than causal effects.‚Äù\nThis shows statistical maturity."
  },
  {
    "objectID": "tidymodels.html#g.-limitations-this-builds-trust",
    "href": "tidymodels.html#g.-limitations-this-builds-trust",
    "title": "Overview of tidymodels",
    "section": "G. Limitations (this builds trust)",
    "text": "G. Limitations (this builds trust)\nAnswer:\nWhat could go wrong or limit generalization?\nAlways include: - Observational data limitations - External validation status - Model interpretability limits\nExample:\n‚ÄúThis analysis used retrospective single-source data, and external validation was not performed. Variable importance reflects predictive contribution rather than causal relevance.‚Äù\nReviewers expect this."
  },
  {
    "objectID": "tidymodels.html#h.-practical-implications-next-steps",
    "href": "tidymodels.html#h.-practical-implications-next-steps",
    "title": "Overview of tidymodels",
    "section": "H. Practical implications / next steps",
    "text": "H. Practical implications / next steps\nAnswer:\nWhat would you do next if this mattered?\nExample:\n‚ÄúFuture work includes external validation, calibration assessment, and comparison with simpler baseline models prior to potential deployment.‚Äù\nThis signals real-world thinking."
  }
]